{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "268b24c1-24da-4fc9-a65e-229e8132db59",
   "metadata": {},
   "source": [
    "# Adaptive RAG with Pathway\n",
    "\n",
    "This notebook shows how you can **dynamically adapt the number of documents in a RAG prompt** using feedback from the LLM. This can give you significant cost reduction of RAG LLM question answering pipelines while maintaining good accuracy. Mistral makes it easy to run Adaptive RAG locally with `ollama` or remotely using the Mistral API.\n",
    "\n",
    "Run this notebook on [Google Colab](https://colab.research.google.com/github/avriiil/cookbook/blob/pathway-adapative-rag/third_party/pathway-adaptive-rag.ipynb).\n",
    "\n",
    "Reference paper: https://pathway.com/developers/showcases/adaptive-rag\n",
    "\n",
    "<img src=\"images/pw-adaptive-rag.png\" width=\"700\"/>\n",
    "\n",
    "Let's jump in! ðŸª‚\n",
    "\n",
    "## Setup\n",
    "\n",
    "### Using APIs \n",
    "\n",
    "* You will need [a Mistral subscription](https://auth.mistral.ai/ui/login?flow=2b98deac-f13f-4e18-a7cd-23ba377a6370) to access the Mistral API.\n",
    "* Create an account and fetch your API Key\n",
    "* Pass the API Key into the prompt in this notebook or set it as environment variable `MISTRAL_API_KEY`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6a2ffd-c3ce-4cb7-a7a5-b32d098807eb",
   "metadata": {},
   "source": [
    "### Installing Libraries\n",
    "You will need to install `pathway` to run this code. We will also install `litellm` and `sentence-transformers` which are optional dependencies of Pathway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860c99fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Uncomment and run if you need to install Pathway and Mistral packages\n",
    "# !pip install -U --prefer-binary \"pathway~=0.9.0\"\n",
    "# !pip install \"litellm>=1.35\"\n",
    "# !pip install -U sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34074f04-96ba-442c-acf7-533fa91d9414",
   "metadata": {},
   "source": [
    "### Accessing Data\n",
    "This notebook uses a sample JSON dataset with ~1000 context from the [SQUAD]() dataset. You can access it from the `data` directory or download it to your own machine using the code below. If you download it to another directory, make sure to update the path in the `documents = pw.io.fs.read(...)` call below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487807eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download `adaptive-rag-contexts.jsonl` with ~1000 contexts from SQUAD dataset\n",
    "# !wget -q -nc https://public-pathway-releases.s3.eu-central-1.amazonaws.com/data/adaptive-rag-contexts.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36fcb8d-7789-46ae-9d56-f07a6c39b782",
   "metadata": {},
   "source": [
    "### Running locally\n",
    "If you want to run this locally (e.g., on your laptop), use [Ollama](https://ollama.ai/library/mistral/tags):\n",
    "\n",
    "* Download [Ollama app](https://ollama.ai/).\n",
    "* Download a `Mistral` model e.g., `ollama pull mistral:instruct`, from various Mistral versions [here](https://ollama.ai/library/mistral) and Mixtral versions [here](https://ollama.ai/library/mixtral) available.\n",
    "* Set flags indicating we will run locally and the Mistral model downloaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93191bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_local = False\n",
    "\n",
    "# # Flags for running locally\n",
    "# run_local = True\n",
    "# local_model = \"ollabma/mistral:instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb0cb60-37e3-448b-a3c8-8d063b6dc608",
   "metadata": {},
   "source": [
    "## Adaptive RAG Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62cb5c7-9e6c-47c3-9fa5-13656dcc1cf2",
   "metadata": {},
   "source": [
    "RAG question-answering applications involve an important trade-off regarding the context size. A large number of documents increases the ability of the LLM to provide a correct answer, but also increases LLM costs, which typically grow linearly with the length of the provided prompt. However, intuitively not all questions are equally hard and some can be answered using a small number of supporting documents, while some may require the LLM to consult a larger prompt. \n",
    "\n",
    "This is where [Adaptive RAG](https://pathway.com/developers/showcases/adaptive-rag) comes in: \n",
    "1. start by asking the model to answer a question using **a small number of documents**.\n",
    "2. if it refuses to answer, we will **increase the context size**.\n",
    "3. we will do this **iteratively until the model returns an answer**. \n",
    "\n",
    "This improves the efficiency of our LLM pipeline. For most queries a single (cheap!) LLM call will be sufficient. A fraction of more complicated questions will require re-asking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7af89fd-d1be-42d7-88db-f7676e0257f9",
   "metadata": {},
   "source": [
    "## Implementing Adaptive RAG\n",
    "\n",
    "Let's implement Adaptive RAG by:\n",
    "1. defining Mistral embedder and LLM\n",
    "2. loading our context documents\n",
    "3. defining our queries\n",
    "4. running Adaptive RAG: to iteratively increase context size\n",
    "5. inspecting the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8926aefe-8b13-4c39-916e-0f5fb8204cfa",
   "metadata": {},
   "source": [
    "Start by importing the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69d538f-3381-44b7-bb60-a4107a7eb67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import pathway as pw\n",
    "from pathway.stdlib.indexing import VectorDocumentIndex\n",
    "from pathway.xpacks.llm.embedders import LiteLLMEmbedder, SentenceTransformerEmbedder \n",
    "from pathway.xpacks.llm.llms import LiteLLMChat  \n",
    "from pathway.xpacks.llm.question_answering import (\n",
    "    answer_with_geometric_rag_strategy_from_index,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97eb829a-b15c-46d4-8803-95bf3228438e",
   "metadata": {},
   "source": [
    "Then define your Mistral embedder and LLM. If you want to run locally with Ollama, make sure to set the correct flags at the top of this notebook.\n",
    "\n",
    "If you are not running locally and have not set the `MISTRAL_API_KEY` environment variable then the following cell will prompt you to securely pass your Mistral API Key.\n",
    "\n",
    "*Note that for the local model:*\n",
    "- we provide tested options for local embedders\n",
    "- we specifically instruct the LLM to return json, which allows the LLM to follow the instructions more strictly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bb7062-4b67-4cf1-beb6-7e548e98d955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check API key\n",
    "if run_local:\n",
    "    pass\n",
    "elif \"MISTRAL_API_KEY\" in os.environ:\n",
    "    mistral_api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
    "else:\n",
    "    mistral_api_key = getpass.getpass(\"Mistral API Key:\")\n",
    "\n",
    "# Set config options\n",
    "embedding_dimension: int = 1024\n",
    "\n",
    "# choose embedder\n",
    "# large_model = \"mixedbread-ai/mxbai-embed-large-v1\"\n",
    "# medium_model = \"avsolatorio/GIST-Embedding-v0\"\n",
    "small_model = \"avsolatorio/GIST-small-Embedding-v0\"\n",
    "\n",
    "# define Mistral embedder\n",
    "if run_local:\n",
    "    embedder = SentenceTransformerEmbedder(small_model, call_kwargs={\"show_progress_bar\": False})  # disable verbose logs\n",
    "else:\n",
    "    embedder = LiteLLMEmbedder(\n",
    "        capacity = 5, \n",
    "        retry_strategy = pw.udfs.FixedDelayRetryStrategy(),\n",
    "        model = \"mistral/mistral-embed\",\n",
    "        api_key=mistral_api_key,\n",
    "    )\n",
    "\n",
    "# define Mistral LLM\n",
    "if run_local:\n",
    "    model = LiteLLMChat(\n",
    "        model=local_model, \n",
    "        temperature=0,\n",
    "        top_p=1,\n",
    "        format=\"json\",  # only available in Ollama local deploy, not usable in Mistral API\n",
    "    )\n",
    "else:\n",
    "    model = LiteLLMChat(\n",
    "        model=\"mistral/mistral-large-latest\", \n",
    "        temperature=0, \n",
    "        api_key=mistral_api_key,\n",
    "        top_p=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c88e17f-ca43-482a-afab-d2afdc06a0b2",
   "metadata": {},
   "source": [
    "Next, let's load the context documents and create a table with our queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38f706c-57b7-4c85-91f5-4a20846fa935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents in which answers will be searched\n",
    "class InputSchema(pw.Schema):\n",
    "    doc: str\n",
    "\n",
    "documents = pw.io.fs.read(\n",
    "    \"data/adaptive-rag-contexts.jsonl\",\n",
    "    format=\"json\",\n",
    "    schema=InputSchema,\n",
    "    json_field_paths={\"doc\": \"/context\"},\n",
    "    mode=\"static\",\n",
    ")\n",
    "\n",
    "# Create table with questions\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"query\": [\n",
    "            \"When it is burned what does hydrogen make?\",\n",
    "            #\"When did Arnold switch from acting to politics?\"\n",
    "            \"What was undertaken in 2010 to determine where dogs originated from?\"\n",
    "            #\"What is a common nickname used to refer to dogs across multiple languages?\",\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "query = pw.debug.table_from_pandas(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfe44ad-5748-4164-9ce9-859d9dd3fb1e",
   "metadata": {},
   "source": [
    "Now let's create a Vector index of the documents and set up our Adaptive RAG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecacdf10-3356-4878-b7e8-c004e0e69655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index for finding closest documents\n",
    "index = VectorDocumentIndex(\n",
    "    documents.doc, documents, embedder, n_dimensions=embedding_dimension\n",
    ")\n",
    "\n",
    "# Run Adaptive RAG\n",
    "result = query.select(\n",
    "    question=query.query,\n",
    "    result=answer_with_geometric_rag_strategy_from_index(\n",
    "        query.query, #define query\n",
    "        index, #pass index\n",
    "        documents.doc, #define context docs\n",
    "        model, #define LLM\n",
    "        n_starting_documents=2, #set number of docs to include in first query iteration\n",
    "        factor=2, #set factor to increase n_docs with\n",
    "        max_iterations=4, #set max number of iterations,\n",
    "        strict_prompt=True,  # needed for open source models, instructs LLM to give JSON output strictly\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b877c454-c490-4647-bb4f-4b1f0f4b7994",
   "metadata": {},
   "source": [
    "Run the cell below to execute Adaptive RAG and fetch the results: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25296ff-6e2a-46ed-beeb-e6f9e14671b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "responses_df = pw.debug.table_to_pandas(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61eeba5d-43c8-41a2-9a9b-232ab22b6b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(responses_df[\"result\"].iloc[0])\n",
    "print(responses_df[\"result\"].iloc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d90c6f",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "We have shown a simple and effective strategy to reduce RAG costs by adapting the number of supporting documents to LLM behavior on a given question. The approach builds on the ability of LLMs to know when they donâ€™t know how to answer. With proper LLM confidence calibration the adaptive RAG is as accurate as a large context base RAG, while being much cheaper to run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440f9049-4cfe-4bff-8018-f0d819390d7e",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Read more about [the technical implementation and benchmarking](https://pathway.com/developers/showcases/adaptive-rag) of Adaptive RAG."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistralll",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
