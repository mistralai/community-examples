{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a RAG application with Milvus Lite, Mistral and Llama-index\n",
    "\n",
    "In this notebook, we are showing how you can build a Retrieval Augmented Generation (RAG) application to interact with data from the French Parliament. It uses Ollama with Mistral for LLM operations, Llama-index for orchestration, and [Milvus](https://milvus.io/) for vector storage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Ollama\n",
    "\n",
    "Make sure to have Ollama installed and Running on your laptop --> https://ollama.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the different dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymilvus==2.5.0 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (2.5.0)\n",
      "Requirement already satisfied: ollama==0.4.4 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (0.4.4)\n",
      "Requirement already satisfied: llama-index-llms-ollama==0.5.0 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (0.5.0)\n",
      "Requirement already satisfied: llama-index-vector-stores-milvus==0.3.0 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (0.3.0)\n",
      "Requirement already satisfied: llama-index-readers-file==0.4.1 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (0.4.1)\n",
      "Requirement already satisfied: llama-index-embeddings-mistralai==0.3.0 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (0.3.0)\n",
      "Collecting llama-index-llms-mistralai==0.3.0\n",
      "  Downloading llama_index_llms_mistralai-0.3.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: setuptools>69 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from pymilvus==2.5.0) (70.0.0)\n",
      "Requirement already satisfied: grpcio<=1.67.1,>=1.49.1 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from pymilvus==2.5.0) (1.63.0)\n",
      "Requirement already satisfied: protobuf>=3.20.0 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from pymilvus==2.5.0) (5.27.1)\n",
      "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from pymilvus==2.5.0) (1.0.1)\n",
      "Requirement already satisfied: ujson>=2.0.0 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from pymilvus==2.5.0) (5.10.0)\n",
      "Requirement already satisfied: pandas>=1.2.4 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from pymilvus==2.5.0) (2.2.2)\n",
      "Requirement already satisfied: milvus-lite>=2.4.0 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from pymilvus==2.5.0) (2.4.7)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from ollama==0.4.4) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from ollama==0.4.4) (2.10.3)\n",
      "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.4 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from llama-index-llms-ollama==0.5.0) (0.12.5)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from llama-index-readers-file==0.4.1) (4.12.3)\n",
      "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from llama-index-readers-file==0.4.1) (5.1.0)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from llama-index-readers-file==0.4.1) (0.0.26)\n",
      "Requirement already satisfied: mistralai>=1.0.0 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from llama-index-embeddings-mistralai==0.3.0) (1.2.5)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file==0.4.1) (2.5)\n",
      "Requirement already satisfied: anyio in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->ollama==0.4.4) (4.4.0)\n",
      "Requirement already satisfied: certifi in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->ollama==0.4.4) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->ollama==0.4.4) (1.0.5)\n",
      "Requirement already satisfied: idna in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->ollama==0.4.4) (3.7)\n",
      "Requirement already satisfied: sniffio in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->ollama==0.4.4) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama==0.4.4) (0.14.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama==0.5.0) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama==0.5.0) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama==0.5.0) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama==0.5.0) (0.6.6)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama==0.5.0) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama==0.5.0) (1.0.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama==0.5.0) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama==0.5.0) (2024.6.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama==0.5.0) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama==0.5.0) (3.3)\n",
      "Requirement already satisfied: nltk>3.8.1 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama==0.5.0) (3.9.1)\n",
      "Requirement already satisfied: numpy in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama==0.5.0) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama==0.5.0) (10.3.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama==0.5.0) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama==0.5.0) (8.3.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama==0.5.0) (0.7.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama==0.5.0) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama==0.5.0) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama==0.5.0) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama==0.5.0) (1.16.0)\n",
      "Requirement already satisfied: eval-type-backport<0.3.0,>=0.2.0 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from mistralai>=1.0.0->llama-index-embeddings-mistralai==0.3.0) (0.2.0)\n",
      "Requirement already satisfied: jsonpath-python<2.0.0,>=1.0.6 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from mistralai>=1.0.0->llama-index-embeddings-mistralai==0.3.0) (1.0.6)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from mistralai>=1.0.0->llama-index-embeddings-mistralai==0.3.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from pandas>=1.2.4->pymilvus==2.5.0) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from pandas>=1.2.4->pymilvus==2.5.0) (2024.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.9.0->ollama==0.4.4) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.9.0->ollama==0.4.4) (2.27.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama==0.5.0) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama==0.5.0) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama==0.5.0) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama==0.5.0) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama==0.5.0) (1.9.4)\n",
      "Requirement already satisfied: click in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama==0.5.0) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama==0.5.0) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama==0.5.0) (2024.5.15)\n",
      "Requirement already satisfied: six>=1.5 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.8.2->mistralai>=1.0.0->llama-index-embeddings-mistralai==0.3.0) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama==0.5.0) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama==0.5.0) (2.2.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama==0.5.0) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama==0.5.0) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama==0.5.0) (3.21.3)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/stephen/Library/Caches/pypoetry/virtualenvs/fork-mistral-cookbook-9DpsfPiA-py3.11/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama==0.5.0) (24.0)\n",
      "Downloading llama_index_llms_mistralai-0.3.0-py3-none-any.whl (6.8 kB)\n",
      "Installing collected packages: llama-index-llms-mistralai\n",
      "Successfully installed llama-index-llms-mistralai-0.3.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pymilvus==2.5.0 ollama==0.4.4 llama-index-llms-ollama==0.5.0 llama-index-vector-stores-milvus==0.3.0 llama-index-readers-file==0.4.1 llama-index-embeddings-mistralai==0.3.0 llama-index-llms-mistralai==0.3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Mistral Embedding\n",
    "\n",
    "Make sure to create an [API Key](https://console.mistral.ai/api-keys/) on Mistral's platform and load it as an environment variable.\n",
    "\n",
    "On this tutorial, we are loading the environment variable stored in our `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "MISTRAL_API_KEY = os.environ.get(\"MISTRAL_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.mistralai import MistralAIEmbedding\n",
    "\n",
    "model_name = \"mistral-embed\"\n",
    "embed_model = MistralAIEmbedding(model_name=model_name, api_key=MISTRAL_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare out data to be stored in Milvus\n",
    "\n",
    "This code makes it possible to process text embeddings using Mistral Embed & Mistral-7B and store those in Milvus.\n",
    "\n",
    "**!!Make sure to have Ollama running on your laptop!!**\n",
    "\n",
    "* Initialises Mistral-7B model using Ollama\n",
    "* Service Context: Configures a service context with Mistral and the embedding model defined above\n",
    "* Vector Store: Sets up a collection in Milvus to store text embeddings, specifying the database file, collection name, vector dimensions\n",
    "* Storage Context: Configures a storage context with the Milvus vector store\n",
    "\n",
    "This makes it possible to have efficient storage and retrieval of vector embeddings for text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.vector_stores.milvus import MilvusVectorStore\n",
    "\n",
    "from llama_index.core import StorageContext, Settings\n",
    "\n",
    "llm = Ollama(model=\"mistral\", request_timeout=120.0)\n",
    "\n",
    "Settings.llm = Ollama(model=\"mistral\", request_timeout=120.0)\n",
    "Settings.embed_model = embed_model\n",
    "Settings.chunk_size = 350\n",
    "Settings.chunk_overlap = 20\n",
    "\n",
    "vector_store = MilvusVectorStore(\n",
    "    uri=\"milvus_mistral_rag.db\",\n",
    "    collection_name=\"mistral_french_parliament\",\n",
    "    dim=1024, \n",
    "    overwrite=True  # drop table if exist and then create\n",
    "    \n",
    "    )\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Mistral AI API\n",
    "\n",
    "If you prefer not to run models locally or need more powerful models, you can use Mistral's API instead of Ollama. The API offers:\n",
    "- Access to more powerful models like `mistral-large` and `mistral-small`\n",
    "- No local GPU/CPU requirements\n",
    "- Consistent performance and reliability\n",
    "- Production-ready deployment\n",
    "\n",
    "Make sure to create an [API Key](https://console.mistral.ai/api-keys/) on Mistral's platform first.\n",
    "```python\n",
    "from llama_index.llms.mistralai import MistralAI\n",
    "\n",
    "# Initialize Mistral LLM\n",
    "mistral_llm = MistralAI(api_key=MISTRAL_API_KEY, model=\"mistral-7B\")\n",
    "\n",
    "# Configure settings for Mistral\n",
    "Settings.llm = mistral_llm\n",
    "```\n",
    "\n",
    "The rest of the setup using Milvus would stay the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process and load the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "\n",
    "docs = SimpleDirectoryReader(input_files=['data/french_parliament_discussion.xml']).load_data()\n",
    "vector_index = VectorStoreIndex.from_documents(docs, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import RetrieverTool, ToolMetadata\n",
    "\n",
    "milvus_tool_openai = RetrieverTool(\n",
    "    retriever=vector_index.as_retriever(similarity_top_k=3),  # retrieve top_k results\n",
    "    metadata=ToolMetadata(\n",
    "        name=\"CustomRetriever\",\n",
    "        description='Retrieve relevant information from provided documents.'\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, ask questions to our RAG system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " In the most recent session discussed in the provided context, the French Parliament talked about the use of the health pass (passe sanitaire). Specifically, there was a debate about the method chosen by the government to implement this pass and concerns were raised about its effectiveness, democracy, and proportionality.\n"
     ]
    }
   ],
   "source": [
    "query_engine = vector_index.as_query_engine()\n",
    "response = query_engine.query(\"What did the French parliament talk about the last time?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you like this tutorial, feel free to reach out on [LinkedIn](https://www.linkedin.com/in/stephen-batifol/), check out [Milvus](https://github.com/milvus-io/milvus) and join our [Discord](https://discord.gg/FG6hMJStWu)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
